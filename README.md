# LLM Security Paper List

## Survey

1. Evaluating Large Language Models: A Comprehensive Survey [[Paper]](https://arxiv.org/abs/2310.19736)

## Hallucination

1. Chain-of-Verification Reduces Hallucination in Large Language Models [[Paper]](https://arxiv.org/abs/2309.11495)
2. DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models [[Paper]](https://arxiv.org/abs/2309.03883)
3. LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples [[Paper]](https://arxiv.org/abs/2310.01469)

## Privacy Protection

1. Large Language Models Can Be Good Privacy Protection Learners [[Paper]](https://arxiv.org/abs/2310.02469)
2. ProPILE: Probing Privacy Leakage in Large Language Models [[Paper]](https://arxiv.org/abs/2307.01881)

## Jailbreak

1. Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study [[Paper]](https://arxiv.org/abs/2305.13860)
2. Jailbroken: How Does LLM Safety Training Fail? [[Paper]](https://arxiv.org/abs/2307.02483)
3. MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots [[Paper]](https://arxiv.org/abs/2307.08715)
4. Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization [[Paper]](https://arxiv.org/abs/2311.09096)
5. Defending ChatGPT against Jailbreak Attack via Self-Reminder [[Paper]](https://www.researchsquare.com/article/rs-2873090/v1)
6. Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM [[Paper]](https://arxiv.org/abs/2309.14348)
7. GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts [[Paper]](https://arxiv.org/abs/2309.10253) [[Code]](https://github.com/sherdencooper/GPTFuzz/tree/master)
8. Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models [[Paper]](https://arxiv.org/abs/2307.08487)
9. Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success [[Paper]](https://arxiv.org/abs/2307.06865)
10. Multi-step Jailbreaking Privacy Attacks on ChatGPT [[Paper]](https://arxiv.org/abs/2304.05197)
11. A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily [[Paper]](https://arxiv.org/abs/2311.08268)
12. DeepInception: Hypnotize Large Language Model to Be Jailbreaker [[Paper]](https://arxiv.org/abs/2311.03191)
13. Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation [[Paper]](https://arxiv.org/abs/2311.03348)
14. Multilingual Jailbreak Challenges in Large Language Models [[Paper]](https://arxiv.org/abs/2310.06474)
15. Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations [[Paper]](https://arxiv.org/abs/2310.06387)
16. AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models [[Paper]](https://arxiv.org/abs/2310.04451)
17. Open Sesame! Universal Black Box Jailbreaking of Large Language Models [[Paper]](https://arxiv.org/abs/2309.01446)
18. SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks [[Paper]](https://aps.arxiv.org/abs/2310.03684)
19. Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks [[Paper]](https://arxiv.org/abs/2305.14965)
20. Universal and Transferable Adversarial Attacks on Aligned Language Models [[Paper]](https://arxiv.org/abs/2307.15043) [[Code]](https://github.com/llm-attacks/llm-attacks?tab=readme-ov-file)

## Datasets

1. [Jailbreak Chat](https://www.jailbreakchat.com/) 


<a href="https://star-history.com/#Dream-Acc/LLM-Security-Paper-List&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Dream-Acc/LLM-Security-Paper-List&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Dream-Acc/LLM-Security-Paper-List&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Dream-Acc/LLM-Security-Paper-List&type=Date" />
  </picture>
</a>
